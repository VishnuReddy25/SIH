{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF gradio"
      ],
      "metadata": {
        "id": "m1jukqef4Rzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import fitz\n",
        "from tqdm.auto import tqdm\n",
        "from spacy.lang.en import English\n",
        "import re\n",
        "import pandas as pd\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "import textwrap\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers.utils import is_flash_attn_2_available\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "pDSpFBzTbtRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MndxcICYuLRM",
        "outputId": "93ca6209-ff22-4fff-b5e2-c6fc2f7fe3f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.10/dist-packages (1.24.13)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.46.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.26.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.1.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.26.2)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.44.1\n",
            "Collecting flash-attn\n",
            "  Downloading flash_attn-2.7.0.post2.tar.gz (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.5.1+cu121)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from flash-attn) (0.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->flash-attn) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (2.1.5)\n",
            "Building wheels for collected packages: flash-attn\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash-attn: filename=flash_attn-2.7.0.post2-cp310-cp310-linux_x86_64.whl size=183291101 sha256=16a849d51b95cf8e47a6e6cd36826e9ffbbc068a8546e7e3501a598bd70905a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/e3/ed/5e845387d52f2debd1bafb847bf3d774d3f0a3c8e31b1dc948\n",
            "Successfully built flash-attn\n",
            "Installing collected packages: flash-attn\n",
            "Successfully installed flash-attn-2.7.0.post2\n"
          ]
        }
      ],
      "source": [
        "# !pip install gTTS googletrans==3.1.0a\n",
        "if \"COLAB_GPU\" in os.environ:\n",
        "    !pip install -U torch\n",
        "    !pip install PyMuPDF # for reading PDFs with Python\n",
        "    !pip install tqdm # for progress bars\n",
        "    !pip install sentence-transformers # for embedding models\n",
        "    !pip install accelerate # for quantization model loading\n",
        "    !pip install bitsandbytes # for quantizing models (less storage space)\n",
        "    !pip install flash-attn --no-build-isolation # for faster attention mechanism = faster LLM inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esAUffqzxzch"
      },
      "outputs": [],
      "source": [
        "pdf_path = \"/content/iesc103.pdf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ppr2rcYUyxLx"
      },
      "outputs": [],
      "source": [
        "def text_formatter(text: str) -> str:\n",
        "    \"\"\"Performs minor formatting on text.\"\"\"\n",
        "    cleaned_text = text.replace(\"\\n\", \" \").strip() # note: this might be different for each doc (best to experiment)\n",
        "\n",
        "    # Other potential text formatting functions can go here\n",
        "    return cleaned_text\n",
        "\n",
        "# Open PDF and get lines/pages\n",
        "# Note: this only focuses on text, rather than images/figures etc\n",
        "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Opens a PDF file, reads its text content page by page, and collects statistics.\n",
        "\n",
        "    Parameters:\n",
        "        pdf_path (str): The file path to the PDF document to be opened and read.\n",
        "\n",
        "    Returns:\n",
        "        list[dict]: A list of dictionaries, each containing the page number\n",
        "        (adjusted), character count, word count, sentence count, token count, and the extracted text\n",
        "        for each page.\n",
        "    \"\"\"\n",
        "    doc = fitz.open(pdf_path)  # open a document\n",
        "    pages_and_texts = []\n",
        "    for page_number, page in tqdm(enumerate(doc)):  # iterate the document pages\n",
        "        text = page.get_text()  # get plain text encoded as UTF-8\n",
        "        text = text_formatter(text)\n",
        "        pages_and_texts.append({\"page_number\": page_number ,  # adjust page numbers\n",
        "                                \"page_char_count\": len(text),\n",
        "                                \"page_word_count\": len(text.split(\" \")),\n",
        "                                \"page_sentence_count_raw\": len(text.split(\". \")),\n",
        "                                \"page_token_count\": len(text) / 4,  # 1 token = ~4 chars, see: https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them\n",
        "                                \"text\": text})\n",
        "    return pages_and_texts\n",
        "\n",
        "pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9ZHAN-xz6d4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # Get stats\n",
        "# df.describe().round(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyjLk8L7z98-"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "nlp = English()\n",
        "\n",
        "# Add a sentencizer pipeline, see https://spacy.io/api/sentencizer/\n",
        "nlp.add_pipe(\"sentencizer\")\n",
        "\n",
        "# # Create a document instance as an example\n",
        "# doc = nlp(\"This is a sentence. This another sentence.\")\n",
        "# assert len(list(doc.sents)) == 2\n",
        "\n",
        "# # Access the sentences of the document\n",
        "# list(doc.sents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHhuobfa0mOh"
      },
      "outputs": [],
      "source": [
        "for item in tqdm(pages_and_texts):\n",
        "    item[\"sentences\"] = list(nlp(item[\"text\"]).sents)\n",
        "\n",
        "    # Make sure all sentences are strings\n",
        "    item[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]\n",
        "\n",
        "    # Count the sentences\n",
        "    item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFxlNDeF1U6_"
      },
      "outputs": [],
      "source": [
        "# Define split size to turn groups of sentences into chunks\n",
        "num_sentence_chunk_size = 10\n",
        "\n",
        "# Create a function that recursively splits a list into desired sizes\n",
        "def split_list(input_list: list,\n",
        "               slice_size: int) -> list[list[str]]:\n",
        "    \"\"\"\n",
        "    Splits the input_list into sublists of size slice_size (or as close as possible).\n",
        "\n",
        "    For example, a list of 17 sentences would be split into two lists of [[10], [7]]\n",
        "    \"\"\"\n",
        "    return [input_list[i:i + slice_size] for i in range(0, len(input_list), slice_size)]\n",
        "\n",
        "# Loop through pages and texts and split sentences into chunks\n",
        "for item in tqdm(pages_and_texts):\n",
        "    item[\"sentence_chunks\"] = split_list(input_list=item[\"sentences\"],\n",
        "                                         slice_size=num_sentence_chunk_size)\n",
        "    item[\"num_chunks\"] = len(item[\"sentence_chunks\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9Kj4rU03s13"
      },
      "outputs": [],
      "source": [
        "# df = pd.DataFrame(pages_and_texts)\n",
        "# df.describe().round(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQxYKe3Z3xdI"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Split each chunk into its own item\n",
        "pages_and_chunks = []\n",
        "for item in tqdm(pages_and_texts):\n",
        "    for sentence_chunk in item[\"sentence_chunks\"]:\n",
        "        chunk_dict = {}\n",
        "        chunk_dict[\"page_number\"] = item[\"page_number\"]\n",
        "\n",
        "        # Join the sentences together into a paragraph-like structure, aka a chunk (so they are a single string)\n",
        "        joined_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \", \" \").strip()\n",
        "        joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk) # \".A\" -> \". A\" for any full-stop/capital letter combo\n",
        "        chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n",
        "\n",
        "        # Get stats about the chunk\n",
        "        chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
        "        chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n",
        "        chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4 # 1 token = ~4 characters\n",
        "\n",
        "        pages_and_chunks.append(chunk_dict)\n",
        "\n",
        "# How many chunks do we have?\n",
        "len(pages_and_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oim5BSYL31Cc"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(pages_and_chunks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrunBZmy38SX"
      },
      "outputs": [],
      "source": [
        "# # Show random chunks with under 30 tokens in length\n",
        "min_token_length = 30\n",
        "# Get the subset of the DataFrame\n",
        "df_subset = df[df[\"chunk_token_count\"] <= min_token_length]\n",
        "# Sample a maximum of 5 rows, or the number of rows in the subset, whichever is smaller\n",
        "num_samples = min(5, len(df_subset))\n",
        "for row in df_subset.sample(num_samples).iterrows():\n",
        "     print(f'Chunk token count: {row[1][\"chunk_token_count\"]} | Text: {row[1][\"sentence_chunk\"]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGx7M0i239mW"
      },
      "outputs": [],
      "source": [
        "pages_and_chunks_over_min_token_len = df[df[\"chunk_token_count\"] > min_token_length].to_dict(orient=\"records\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jthturPh4G6k"
      },
      "outputs": [],
      "source": [
        "# Requires !pip install sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\",\n",
        "                                      device=\"cuda\") # choose the device to load the model to (note: GPU will often be *much* faster than CPU)\n",
        "\n",
        "# Create a list of sentences to turn into numbers\n",
        "sentences = [\n",
        "    \"The Sentences Transformers library provides an easy and open-source way to create embeddings.\",\n",
        "    \"Sentences can be embedded one by one or as a list of strings.\",\n",
        "    \"Embeddings are one of the most powerful concepts in machine learning!\",\n",
        "    \"Learn to use embeddings well and you'll be well on your way to being an AI engineer.\"\n",
        "]\n",
        "\n",
        "# Sentences are encoded/embedded by calling model.encode()\n",
        "embeddings = embedding_model.encode(sentences)\n",
        "embeddings_dict = dict(zip(sentences, embeddings))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WRgO8K54Saq"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# Send the model to the GPU\n",
        "embedding_model.to(\"cuda\") #\n",
        "\n",
        "# Create embeddings one by one on the GPU\n",
        "for item in tqdm(pages_and_chunks_over_min_token_len):\n",
        "    item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqzkcVz-5OEJ"
      },
      "outputs": [],
      "source": [
        "# Turn text chunks into a single list\n",
        "text_chunks = [item[\"sentence_chunk\"] for item in pages_and_chunks_over_min_token_len]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4qxK2uh5Q1v"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# Embed all texts in batches\n",
        "text_chunk_embeddings = embedding_model.encode(text_chunks,\n",
        "                                               batch_size=32, # you can use different batch sizes here for speed/performance, I found 32 works well for this use case\n",
        "                                               convert_to_tensor=True) # optional to return embeddings as tensor instead of array\n",
        "\n",
        "# text_chunk_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reFNjSlg5cAx"
      },
      "outputs": [],
      "source": [
        "# Save embeddings to file\n",
        "text_chunks_and_embeddings_df = pd.DataFrame(pages_and_chunks_over_min_token_len)\n",
        "embeddings_df_save_path = \"text_chunks_and_embeddings_df.csv\"\n",
        "text_chunks_and_embeddings_df.to_csv(embeddings_df_save_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGC5nXeq6fy7"
      },
      "outputs": [],
      "source": [
        "# Import saved file and view\n",
        "text_chunks_and_embedding_df_load = pd.read_csv(embeddings_df_save_path)\n",
        "# text_chunks_and_embedding_df_load.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ara_z9B6hOz"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Import texts and embedding df\n",
        "text_chunks_and_embedding_df = pd.read_csv(\"text_chunks_and_embeddings_df.csv\")\n",
        "\n",
        "# Convert embedding column back to np.array (it got converted to string when it got saved to CSV)\n",
        "text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
        "\n",
        "# Convert texts and embedding df to list of dicts\n",
        "pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n",
        "\n",
        "# Convert embeddings to torch tensor and send to device (note: NumPy arrays are float64, torch tensors are float32 by default)\n",
        "embeddings = torch.tensor(np.array(text_chunks_and_embedding_df[\"embedding\"].tolist()), dtype=torch.float32).to(device)\n",
        "# embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vN6mCjD84M6"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import util, SentenceTransformer\n",
        "\n",
        "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\",\n",
        "                                      device=device) # choose the device to load the model to"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oy6RYSc79M1-"
      },
      "outputs": [],
      "source": [
        "def print_wrapped(text, wrap_length=80):\n",
        "    wrapped_text = textwrap.fill(text, wrap_length)\n",
        "    print(wrapped_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rx3j4z119Wa9"
      },
      "outputs": [],
      "source": [
        "\n",
        "query = \"what are fundamental rights\"\n",
        "# Open PDF and load target page\n",
        "pdf_path = \"/content/iesc103.pdf\"\n",
        "doc = fitz.open(pdf_path)\n",
        "page = doc.load_page(5) # number of page (our doc starts page numbers on page 41)\n",
        "\n",
        "# Get the image of the page\n",
        "img = page.get_pixmap(dpi=300)\n",
        "\n",
        "# Optional: save the image\n",
        "#img.save(\"output_filename.png\")\n",
        "doc.close()\n",
        "\n",
        "# Convert the Pixmap to a numpy array\n",
        "img_array = np.frombuffer(img.samples_mv,\n",
        "                          dtype=np.uint8).reshape((img.h, img.w, img.n))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7PymOTwGl6y"
      },
      "outputs": [],
      "source": [
        "from time import perf_counter as timer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdvgpxHq93Qn"
      },
      "outputs": [],
      "source": [
        "def retrieve_relevant_resources(query: str,\n",
        "                                embeddings: torch.tensor,\n",
        "                                model: SentenceTransformer=embedding_model,\n",
        "                                n_resources_to_return: int=5,\n",
        "                                print_time: bool=True):\n",
        "    \"\"\"\n",
        "    Embeds a query with model and returns top k scores and indices from embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    # Embed the query\n",
        "    query_embedding = model.encode(query,\n",
        "                                   convert_to_tensor=True)\n",
        "\n",
        "    # Get dot product scores on embeddings\n",
        "    start_time = timer()\n",
        "    dot_scores = util.dot_score(query_embedding, embeddings)[0]\n",
        "    end_time = timer()\n",
        "\n",
        "    if print_time:\n",
        "        print(f\"[INFO] Time taken to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n",
        "\n",
        "    scores, indices = torch.topk(input=dot_scores,\n",
        "                                 k=n_resources_to_return)\n",
        "\n",
        "    return scores, indices\n",
        "\n",
        "def print_top_results_and_scores(query: str,\n",
        "                                 embeddings: torch.tensor,\n",
        "                                 pages_and_chunks: list[dict]=pages_and_chunks,\n",
        "                                 n_resources_to_return: int=5):\n",
        "    \"\"\"\n",
        "    Takes a query, retrieves most relevant resources and prints them out in descending order.\n",
        "\n",
        "    Note: Requires pages_and_chunks to be formatted in a specific way (see above for reference).\n",
        "    \"\"\"\n",
        "\n",
        "    scores, indices = retrieve_relevant_resources(query=query,\n",
        "                                                  embeddings=embeddings,\n",
        "                                                  n_resources_to_return=n_resources_to_return)\n",
        "\n",
        "    print(f\"Query: {query}\\n\")\n",
        "    print(\"Results:\")\n",
        "    # Loop through zipped together scores and indicies\n",
        "    for score, index in zip(scores, indices):\n",
        "        print(f\"Score: {score:.4f}\")\n",
        "        # Print relevant sentence chunk (since the scores are in descending order, the most relevant chunk will be first)\n",
        "        print_wrapped(pages_and_chunks[index][\"sentence_chunk\"])\n",
        "        # Print the page number too so we can reference the textbook further and check the results\n",
        "        print(f\"Page number: {pages_and_chunks[index]['page_number']}\")\n",
        "        print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Po9bYeAZ9-o2"
      },
      "outputs": [],
      "source": [
        "# # Get GPU available memory\n",
        "# import torch\n",
        "# gpu_memory_bytes = torch.cuda.get_device_properties(0).total_memory\n",
        "# gpu_memory_gb = round(gpu_memory_bytes / (2**30))\n",
        "# print(f\"Available GPU memory: {gpu_memory_gb} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAb7PAuPNu61"
      },
      "outputs": [],
      "source": [
        "# if gpu_memory_gb < 5.1:\n",
        "#     print(f\"Your available GPU memory is {gpu_memory_gb}GB, you may not have enough memory to run a Gemma LLM locally without quantization.\")\n",
        "# elif gpu_memory_gb < 8.1:\n",
        "#     print(f\"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in 4-bit precision.\")\n",
        "#     use_quantization_config = True\n",
        "#     model_id = \"google/gemma-2b-it\"\n",
        "# elif gpu_memory_gb < 19.0:\n",
        "#     print(f\"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in float16 or Gemma 7B in 4-bit precision.\")\n",
        "#     use_quantization_config = False\n",
        "#     model_id = \"google/gemma-2b-it\"\n",
        "# elif gpu_memory_gb > 19.0:\n",
        "#     print(f\"GPU memory: {gpu_memory_gb} | Recommend model: Gemma 7B in 4-bit or float16 precision.\")\n",
        "#     use_quantization_config = False\n",
        "#     model_id = \"google/gemma-7b-it\"\n",
        "\n",
        "# print(f\"use_quantization_config set to: {use_quantization_config}\")\n",
        "# print(f\"model_id set to: {model_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4z2pSSaW0Piw"
      },
      "outputs": [],
      "source": [
        "use_quantization_config = False\n",
        "model_id = \"google/gemma-2b-it\"\n",
        "# model_id = \"google/datagemma-rag-27b-it\"\n",
        "# model_id = \"google/gemma-7b-it\"\n",
        "# model_id=\"Telugu-LLM-Labs/Indic-gemma-2b-finetuned-sft-Navarasa-2.0\"\n",
        "# model_id=\"meta-llama/Llama-3.2-3B\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5J4iGfJOEFk"
      },
      "outputs": [],
      "source": [
        "# 1. Create quantization config for smaller model loading (optional)\n",
        "# Requires !pip install bitsandbytes accelerate, see: https://github.com/TimDettmers/bitsandbytes, https://huggingface.co/docs/accelerate/\n",
        "# For models that require 4-bit quantization (use this if you have low GPU memory available)\n",
        "from transformers import BitsAndBytesConfig\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
        "                                         bnb_4bit_compute_dtype=torch.float16)\n",
        "\n",
        "# Bonus: Setup Flash Attention 2 for faster inference, default to \"sdpa\" or \"scaled dot product attention\" if it's not available\n",
        "# Flash Attention 2 requires NVIDIA GPU compute capability of 8.0 or above, see: https://developer.nvidia.com/cuda-gpus\n",
        "# Requires !pip install flash-attn, see: https://github.com/Dao-AILab/flash-attention\n",
        "if (is_flash_attn_2_available()) and (torch.cuda.get_device_capability(0)[0] >= 8):\n",
        "  attn_implementation = \"flash_attention_2\"\n",
        "else:\n",
        "  attn_implementation = \"sdpa\"\n",
        "print(f\"[INFO] Using attention implementation: {attn_implementation}\")\n",
        "\n",
        "# 2. Pick a model we'd like to use (this will depend on how much GPU memory you have available)\n",
        "#model_id = \"google/gemma-7b-it\"\n",
        "model_id = model_id # (we already set this above)\n",
        "print(f\"[INFO] Using model_id: {model_id}\")\n",
        "!huggingface-cli login\n",
        "\n",
        "# 3. Instantiate tokenizer (tokenizer turns text into numbers ready for the model)\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n",
        "\n",
        "# 4. Instantiate the model\n",
        "llm_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id,\n",
        "                                                 torch_dtype=torch.float16, # datatype to use, we want float16\n",
        "                                                 quantization_config=quantization_config if use_quantization_config else None,\n",
        "                                                 low_cpu_mem_usage=False, # use full memory\n",
        "                                                 attn_implementation=attn_implementation) # which attention version to use\n",
        "\n",
        "if not use_quantization_config: # quantization takes care of device setting automatically, so if it's not used, send model to GPU\n",
        "    llm_model.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vcU0icGRp0A"
      },
      "outputs": [],
      "source": [
        "def get_model_num_params(model: torch.nn.Module):\n",
        "    return sum([param.numel() for param in model.parameters()])\n",
        "\n",
        "get_model_num_params(llm_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoWmwyT662xz"
      },
      "outputs": [],
      "source": [
        "llm_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XamMvlYtRxjw"
      },
      "outputs": [],
      "source": [
        "def get_model_mem_size(model: torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Get how much memory a PyTorch model takes up.\n",
        "\n",
        "    See: https://discuss.pytorch.org/t/gpu-memory-that-model-uses/56822\n",
        "    \"\"\"\n",
        "    # Get model parameters and buffer sizes\n",
        "    mem_params = sum([param.nelement() * param.element_size() for param in model.parameters()])\n",
        "    mem_buffers = sum([buf.nelement() * buf.element_size() for buf in model.buffers()])\n",
        "\n",
        "    # Calculate various model sizes\n",
        "    model_mem_bytes = mem_params + mem_buffers # in bytes\n",
        "    model_mem_mb = model_mem_bytes / (1024**2) # in megabytes\n",
        "    model_mem_gb = model_mem_bytes / (1024**3) # in gigabytes\n",
        "\n",
        "    return {\"model_mem_bytes\": model_mem_bytes,\n",
        "            \"model_mem_mb\": round(model_mem_mb, 2),\n",
        "            \"model_mem_gb\": round(model_mem_gb, 2)}\n",
        "\n",
        "get_model_mem_size(llm_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IH3f7Kl15Ka"
      },
      "outputs": [],
      "source": [
        "def prompt_formatter(query: str,\n",
        "                     context_items: list[dict]) -> str:\n",
        "    \"\"\"\n",
        "    Augments query with text-based context from context_items.\n",
        "    \"\"\"\n",
        "    # Join context items into one dotted paragraph\n",
        "    context = \"- \" + \"\\n- \".join([item[\"sentence_chunk\"] for item in context_items])\n",
        "\n",
        "    # Create a base prompt with examples to help the model\n",
        "    # Note: this is very customizable, I've chosen to use 3 examples of the answer style we'd like.\n",
        "    # We could also write this in a txt file and import it in if we wanted.\n",
        "    base_prompt = \"\"\"Based on the following context items, please answer the query.\n",
        "Give yourself room to think by extracting relevant passages from the context before answering the query.\n",
        "Don't return the thinking, only return the answer.\n",
        "Make sure your answers are as explanatory as possible.\n",
        "Use the following examples as reference for the ideal answer style.\n",
        "\\nExample 1:\n",
        "Query: What are the fat-soluble vitamins?\n",
        "Answer: The fat-soluble vitamins include Vitamin A, Vitamin D, Vitamin E, and Vitamin K. These vitamins are absorbed along with fats in the diet and can be stored in the body's fatty tissue and liver for later use. Vitamin A is important for vision, immune function, and skin health. Vitamin D plays a critical role in calcium absorption and bone health. Vitamin E acts as an antioxidant, protecting cells from damage. Vitamin K is essential for blood clotting and bone metabolism.\n",
        "\\nExample 2:\n",
        "Query: What are the causes of type 2 diabetes?\n",
        "Answer: Type 2 diabetes is often associated with overnutrition, particularly the overconsumption of calories leading to obesity. Factors include a diet high in refined sugars and saturated fats, which can lead to insulin resistance, a condition where the body's cells do not respond effectively to insulin. Over time, the pancreas cannot produce enough insulin to manage blood sugar levels, resulting in type 2 diabetes. Additionally, excessive caloric intake without sufficient physical activity exacerbates the risk by promoting weight gain and fat accumulation, particularly around the abdomen, further contributing to insulin resistance.\n",
        "\\nExample 3:\n",
        "Query: What is the importance of hydration for physical performance?\n",
        "Answer: Hydration is crucial for physical performance because water plays key roles in maintaining blood volume, regulating body temperature, and ensuring the transport of nutrients and oxygen to cells. Adequate hydration is essential for optimal muscle function, endurance, and recovery. Dehydration can lead to decreased performance, fatigue, and increased risk of heat-related illnesses, such as heat stroke. Drinking sufficient water before, during, and after exercise helps ensure peak physical performance and recovery.\n",
        "\\nNow use the following context items to answer the user query:\n",
        "{context}\n",
        "\\nRelevant passages: <extract relevant passages from the context here>\n",
        "User query: {query}\n",
        "Answer:\"\"\"\n",
        "\n",
        "    # Update base prompt with context items and query\n",
        "    base_prompt = base_prompt.format(context=context, query=query)\n",
        "\n",
        "    # Create prompt template for instruction-tuned model\n",
        "    dialogue_template = [\n",
        "        {\"role\": \"user\",\n",
        "        \"content\": base_prompt}\n",
        "    ]\n",
        "\n",
        "    # Apply the chat template\n",
        "    prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
        "                                          tokenize=False,\n",
        "                                          add_generation_prompt=True)\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USgaJzaXSy6P"
      },
      "outputs": [],
      "source": [
        "def ask(query,\n",
        "        temperature=0.7,\n",
        "        max_new_tokens=512,\n",
        "        format_answer_text=True,\n",
        "        return_answer_only=True):\n",
        "    \"\"\"\n",
        "    Takes a query, finds relevant resources/context and generates an answer to the query based on the relevant resources.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get just the scores and indices of top related results\n",
        "    scores, indices = retrieve_relevant_resources(query=query,\n",
        "                                                  embeddings=embeddings)\n",
        "\n",
        "    # Create a list of context items\n",
        "    context_items = [pages_and_chunks[i] for i in indices]\n",
        "\n",
        "    # Add score to context item\n",
        "    for i, item in enumerate(context_items):\n",
        "        item[\"score\"] = scores[i].cpu() # return score back to CPU\n",
        "\n",
        "    # Format the prompt with context items\n",
        "    prompt = prompt_formatter(query=query,\n",
        "                              context_items=context_items)\n",
        "\n",
        "    # Tokenize the prompt\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Generate an output of tokens\n",
        "    outputs = llm_model.generate(**input_ids,\n",
        "                                 temperature=temperature,\n",
        "                                 do_sample=True,\n",
        "                                 max_new_tokens=max_new_tokens)\n",
        "\n",
        "    # Turn the output tokens into text\n",
        "    output_text = tokenizer.decode(outputs[0])\n",
        "\n",
        "    if format_answer_text:\n",
        "        # Replace special tokens and unnecessary help message\n",
        "        output_text = output_text.replace(prompt, \"\").replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").replace(\"Sure, here is the answer to the user query:\\n\\n\", \"\")\n",
        "\n",
        "    # Only return the answer without the context items\n",
        "    if return_answer_only:\n",
        "        return output_text\n",
        "\n",
        "    return output_text, context_items"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "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"
      ],
      "metadata": {
        "id": "Zjy0ISfeVsC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3iplUcnpLWw"
      },
      "outputs": [],
      "source": [
        "# !pip install speechbrain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9V-HTEYpQwZ"
      },
      "outputs": [],
      "source": [
        "# import torchaudio\n",
        "# from speechbrain.inference.TTS import Tacotron2\n",
        "# from speechbrain.inference.vocoders import HIFIGAN\n",
        "\n",
        "# # Intialize TTS (tacotron2) and Vocoder (HiFIGAN)\n",
        "# tacotron2 = Tacotron2.from_hparams(source=\"speechbrain/tts-tacotron2-ljspeech\", savedir=\"tmpdir_tts\")\n",
        "# hifi_gan = HIFIGAN.from_hparams(source=\"speechbrain/tts-hifigan-ljspeech\", savedir=\"tmpdir_vocoder\")\n",
        "\n",
        "# # Running the TTS\n",
        "# mel_output, mel_length, alignment = tacotron2.encode_text(\"Mary had a little lamb\")\n",
        "\n",
        "# # Running Vocoder (spectrogram-to-waveform)\n",
        "# waveforms = hifi_gan.decode_batch(mel_output)\n",
        "\n",
        "# # Save the waverform\n",
        "# torchaudio.save('example_TTS.wav',waveforms.squeeze(1), 22050)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STs9PJaSpafZ"
      },
      "outputs": [],
      "source": [
        "# from speechbrain.pretrained import Tacotron2\n",
        "# tacotron2 = Tacotron2.from_hparams(source=\"speechbrain/TTS_Tacotron2\", savedir=\"tmpdir\")\n",
        "# items = [\n",
        "#        \"hello\"\n",
        "#      ]\n",
        "# mel_outputs, mel_lengths, alignments = tacotron2.encode_batch(items)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXLdcxxnpkX2"
      },
      "outputs": [],
      "source": [
        "# from IPython.display import Audio\n",
        "\n",
        "# # Assuming 'waveforms' contains the audio data generated earlier\n",
        "# Audio(waveforms.squeeze().numpy(), rate=22050) # squeeze the tensor to remove extra dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSDwCWVmp7ig"
      },
      "outputs": [],
      "source": [
        "# # prompt: i am getting some other speech and not hello\n",
        "\n",
        "# tacotron2 = Tacotron2.from_hparams(source=\"speechbrain/TTS_Tacotron2\", savedir=\"tmpdir\")\n",
        "# items = [\n",
        "#        \"hello\"\n",
        "#      ]\n",
        "# mel_outputs, mel_lengths, alignments = tacotron2.encode_batch(items)\n",
        "# waveforms = hifi_gan.decode_batch(mel_outputs)\n",
        "# Audio(waveforms.squeeze().numpy(), rate=22050)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07aQTPvXqFuG"
      },
      "outputs": [],
      "source": [
        "\n",
        "# tacotron2 = Tacotron2.from_hparams(source=\"speechbrain/tts-tacotron2-ljspeech\", savedir=\"tmpdir_tts\")\n",
        "# hifi_gan = HIFIGAN.from_hparams(source=\"speechbrain/tts-hifigan-ljspeech\", savedir=\"tmpdir_vocoder\")\n",
        "\n",
        "\n",
        "# def predict_with_tts(message, history):\n",
        "#     # Use your existing ask function to generate a response\n",
        "#     bot_message = ask(query=message,\n",
        "#                       temperature=0.7,\n",
        "#                       max_new_tokens=512,\n",
        "#                       return_answer_only=True)\n",
        "\n",
        "#     # Generate speech from the bot's response\n",
        "#     mel_output, mel_length, alignment = tacotron2.encode_text(bot_message)\n",
        "#     waveforms = hifi_gan.decode_batch(mel_output)\n",
        "\n",
        "#     # Save the waveform temporarily\n",
        "#     torchaudio.save('temp_audio.wav', waveforms.squeeze(1), 22050)\n",
        "\n",
        "#     return bot_message, \"temp_audio.wav\"  # Return both text and audio file path\n",
        "\n",
        "\n",
        "# with gr.Blocks() as demo:\n",
        "#     chatbot = gr.ChatInterface(predict_with_tts,\n",
        "#                                title=\"My Chatbot with TTS\",\n",
        "#                                description=\"Ask me anything!\",\n",
        "#                                examples=[\"give summary on fasttrack courts\"],\n",
        "#                                # output_components=[gr.Textbox(), gr.Audio(label=\"Audio Response\")],\n",
        "#                                )\n",
        "# demo.launch(share=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEcoG-16rInv"
      },
      "outputs": [],
      "source": [
        "# # prompt: i need my chatbot to play the audio\n",
        "\n",
        "# from google.colab import output\n",
        "# from IPython.display import Audio\n",
        "# import os\n",
        "\n",
        "# def predict_with_tts(message, history):\n",
        "#     # Use your existing ask function to generate a response\n",
        "#     bot_message = ask(query=message,\n",
        "#                       temperature=0.7,\n",
        "#                       max_new_tokens=512,\n",
        "#                       return_answer_only=True)\n",
        "\n",
        "#     # Generate speech from the bot's response\n",
        "#     mel_output, mel_length, alignment = tacotron2.encode_text(bot_message)\n",
        "#     waveforms = hifi_gan.decode_batch(mel_output)\n",
        "\n",
        "#     # Play the audio directly in the notebook\n",
        "#     output.eval_js('new Audio(\"data:audio/wav;base64,\" + btoa(String.fromCharCode(...new Uint8Array(({}))))).play()',\n",
        "#                   waveforms.squeeze().numpy().tobytes())\n",
        "\n",
        "#     return bot_message\n",
        "\n",
        "\n",
        "# with gr.Blocks() as demo:\n",
        "#     chatbot = gr.ChatInterface(predict_with_tts,\n",
        "#                                title=\"My Chatbot with TTS\",\n",
        "#                                description=\"Ask me anything!\",\n",
        "#                                examples=[\"give summary on fasttrack courts\"])\n",
        "# demo.launch(share=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHRPE2MDrXBY"
      },
      "outputs": [],
      "source": [
        "# # prompt: chatbot along with audio feature code\n",
        "\n",
        "# import os\n",
        "# import requests\n",
        "# import fitz\n",
        "# from tqdm.auto import tqdm\n",
        "# from spacy.lang.en import English # see https://spacy.io/usage for install instructions\n",
        "# import re\n",
        "# import pandas as pd\n",
        "# import random\n",
        "# import torch\n",
        "# import numpy as np\n",
        "# import textwrap\n",
        "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "# from transformers.utils import is_flash_attn_2_available\n",
        "# import gradio as gr\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "# from sentence_transformers import util, SentenceTransformer\n",
        "# from time import perf_counter as timer\n",
        "# from transformers import BitsAndBytesConfig\n",
        "# import torchaudio\n",
        "# from speechbrain.inference.TTS import Tacotron2\n",
        "# from speechbrain.inference.vocoders import HIFIGAN\n",
        "# from speechbrain.pretrained import Tacotron2\n",
        "# from IPython.display import Audio\n",
        "# from google.colab import output\n",
        "\n",
        "# # ... (rest of your existing code) ...\n",
        "\n",
        "# def predict_with_tts(message, history):\n",
        "#     # Use your existing ask function to generate a response\n",
        "#     bot_message = ask(query=message,\n",
        "#                       temperature=0.7,\n",
        "#                       max_new_tokens=512,\n",
        "#                       return_answer_only=True)\n",
        "\n",
        "#     # Generate speech from the bot's response\n",
        "#     mel_output, mel_length, alignment = tacotron2.encode_text(bot_message)\n",
        "#     waveforms = hifi_gan.decode_batch(mel_output)\n",
        "\n",
        "#     # Play the audio directly in the notebook\n",
        "#     output.eval_js('new Audio(\"data:audio/wav;base64,\" + btoa(String.fromCharCode(...new Uint8Array(({}))))).play()',\n",
        "#                   waveforms.squeeze().numpy().tobytes())\n",
        "\n",
        "#     return bot_message\n",
        "\n",
        "\n",
        "# with gr.Blocks() as demo:\n",
        "#     chatbot = gr.ChatInterface(predict_with_tts,\n",
        "#                                title=\"My Chatbot with TTS\",\n",
        "#                                description=\"Ask me anything!\",\n",
        "#                                examples=[\"give summary on fasttrack courts\"])\n",
        "# demo.launch(share=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok\n"
      ],
      "metadata": {
        "id": "J3LC1cO7wMg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask-ngrok\n"
      ],
      "metadata": {
        "id": "UTqhMOxI1CJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Manually install ngrok\n",
        "!wget -O ngrok.zip https://bin.equinox.io/c/111111/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok.zip\n",
        "!./ngrok http 5000  # Use the correct port\n"
      ],
      "metadata": {
        "id": "w46sqOsS1LKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from flask_cors import CORS\n",
        "\n",
        "# Initialize the Flask app\n",
        "app = Flask(__name__)\n",
        "CORS(app)  # Enable CORS for all routes\n",
        "run_with_ngrok(app)  # Start ngrok when the app runs\n",
        "\n",
        "# Define your chatbot route\n",
        "@app.route('/chatbot', methods=['POST'])\n",
        "def chatbot():\n",
        "    data = request.json\n",
        "    user_input = data.get('message')\n",
        "\n",
        "    # Call your existing ask function\n",
        "    if user_input:\n",
        "        response = ask(query=user_input)  # Assuming you have your ask function defined\n",
        "    else:\n",
        "        response = \"I didn't understand that.\"\n",
        "\n",
        "    return jsonify({\"response\": response})\n",
        "\n",
        "# Run the Flask app\n",
        "if __name__ == \"__main__\":\n",
        "    app.run()\n"
      ],
      "metadata": {
        "id": "czCZ7aI61Sys"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}